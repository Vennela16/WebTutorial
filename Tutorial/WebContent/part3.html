<html>
<head>
    <title>Implementation of Web Content Mining</title>
      <link rel="stylesheet" href="style.css">
    <style>
        
    </style>
</head>
<body>
    <table style="width:100%">
        <tr>
            <th><strong>Back:</strong> <a href="part2.html">The Information Retrieval View of Web Content Mining</a></th><th><strong>Up:</strong> <a href="First.html">Index</a></th><th><strong>Next:</strong><a href="part4.html">Algorithm Used for Content Mining</a></th>
        </tr>
    </table>
    <h1 class="centered-text"><strong>Implementation of Web Content Mining</strong></h1>
    <h3>1)  Scraping</h3>
    <p class="tab">
        The first step of web content mining is web scraping: getting the information that we need from a web page. Scraping is simultaneously the most important and most straightforward part of web content mining. It requires the application to determine what is important and what is not; in other words, which features it will need to extract for later classification.  This can be done manually if one knows the layout of the sites beforehand: the developer only needs to specify what elements should be extracted, through their appropriate CSS, XPath elements, or other identifier.  For more advanced applications, or applications where the layout of the site is not known, type of machine learning called “wrapper induction” to determine which features to extract. Wrapper induction uses supervised learning to generate a list of rules that cover all of the features we are looking for without covering anything else.  There is also an alternative to wrapper induction called automatic feature extraction, which generates a wrapper from a sample page and refining it across multiple pages as mismatches arise.  However, AFE carries a high computational cost (exponential time) and has several problems, including difficulty in generating attribute names and needing integration if extracting data from multiple sites.  In any case, we figure out the wrapper and feed it to the scraper to extract the features we want.
    </p>
    
    <p class="tab">
        For our application, we used the scraping tool known as Portia, which provides a GUI interface through which to use Scrapy.  We simply select the kinds of elements that we wish to extract, and Portia will automatically extract them based on their HTML/CSS tag.  A demo is shown below.
    </p>
    <div class="center">
        <video width="500" height="300" controls>
            <source src="PortiaDemo.mp4" type=video/mp4>
        </video>
    </div>
    
    <h3>2)  Operating on the scraped data</h3>
    <p class="tab">
        Once we have successfully extracted the data we want, we can now further extract useful information from it.  The method of extraction varies widely with the implementation: for some applications, one has to use advanced algorithms to automatically extract the information needed (some of which will be expanded upon in the next section).  However, for our application, searching StackOverflow to find the most common problems in SQL, the simplest implementation would be to simply search the extracted data for a string pattern; for example, we can search the above example output for the string "SQL" to find information about popular SQL problems that people ask about on Stack Overflow.
    </p>
    
    <div class="center">
        <figure style="text-align: center">
                <img src="Search_for_SQL.png" width="500px"/>
                <figcaption style="text-align: center">Searching the mined data for "SQL"</figcaption>
        </figure>    
    </div> 
    <p class="tab">
        Another simple implementation for our application would be to convert the JSON lines output file to SQL or another database language, with a tool such as SQLizer.  This would allow us to perform more complex queries on the data set.  Once again, it all depends on the application.
    </p>
    </body>
    </html>